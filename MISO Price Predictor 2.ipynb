{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485539a3-15dc-45de-9afb-0d277f8db6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Using pip\n",
    "#!pip install torch torchvision torchaudio\n",
    "\n",
    "# Or using conda\n",
    "#!conda install pytorch torchvision torchaudio -c pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01550b25-3a0f-4339-b0ce-8b683bc92be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind data columns:\n",
      "['UTC Timestamp (Interval Ending)', 'Local Timestamp Eastern Standard Time (Interval Beginning)', 'Local Timestamp Eastern Standard Time (Interval Ending)', 'Local Date', 'Hour Number', 'MISO Total Wind Generation (MW)', 'source_file']\n",
      "\n",
      "Temperature data columns:\n",
      "['Minneapolis Temperature (Fahrenheit)', 'Minneapolis Temperature Observation Time (Eastern Standard)', 'UTC Timestamp (Interval Ending)', 'Local Timestamp Eastern Standard Time (Interval Beginning)', 'Local Timestamp Eastern Standard Time (Interval Ending)', 'Local Date', 'Hour Number', 'source_file']\n",
      "\n",
      "LMP data columns:\n",
      "['UTC Timestamp (Interval Ending)', 'Local Timestamp Eastern Standard Time (Interval Beginning)', 'Local Timestamp Eastern Standard Time (Interval Ending)', 'Local Date', 'Hour Number', 'Minnesota Hub LMP', 'Minnesota Hub (Congestion)', 'Minnesota Hub (Loss)', 'source_file']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files and print their column names\n",
    "wind_df = pd.read_csv('combined_wind_data.csv')\n",
    "lmp_df = pd.read_csv('miso_lmp_hourly_data_minnesota.csv')\n",
    "temp_df = pd.read_csv('temp_data_minnesota.csv')\n",
    "\n",
    "print(\"Wind data columns:\")\n",
    "print(wind_df.columns.tolist())\n",
    "print(\"\\nTemperature data columns:\")\n",
    "print(temp_df.columns.tolist())\n",
    "print(\"\\nLMP data columns:\")\n",
    "print(lmp_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5282141-1283-4c26-bcb3-aff6d6e453ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading datasets...\n",
      "Processing timestamps...\n",
      "Filtering data from 2020-01-01 00:00:00 to 2023-12-31 23:00:00...\n",
      "Extracting data...\n",
      "\n",
      "Dataset information:\n",
      "Time range: 2020-01-01 00:00:00 to 2023-12-31 23:00:00\n",
      "Number of records: 35064\n",
      "Preparing features...\n",
      "Creating features...\n",
      "Creating lagged features...\n",
      "Creating rolling statistics...\n",
      "\n",
      "Final feature set shape: (24941, 595)\n",
      "Number of features created: 594\n",
      "Splitting data...\n",
      "Selecting important features...\n",
      "\n",
      "Selected 7 important features out of 594\n",
      "\n",
      "Training models...\n",
      "Training Random Forest model...\n",
      "Training Gradient Boosting model...\n",
      "\n",
      "Random Forest Evaluation:\n",
      "\n",
      "Model Performance Metrics:\n",
      "MAE: $9.27\n",
      "RMSE: $23.08\n",
      "MAPE: 50.45%\n",
      "R2 Score: 0.331\n",
      "\n",
      "Prediction Interval (68% confidence): ±$23.03\n",
      "Prediction Interval (95% confidence): ±$46.07\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                         feature  importance\n",
      "0                   price_lag_1h    0.830257\n",
      "4  temperature_rolling_mean_168h    0.082318\n",
      "1                    loss_lag_1h    0.034258\n",
      "6               solar_gen_lag_4h    0.024172\n",
      "3                    loss_lag_8h    0.013376\n",
      "2              congestion_lag_6h    0.009587\n",
      "5              congestion_lag_7h    0.006033\n",
      "\n",
      "Gradient Boosting Evaluation:\n",
      "\n",
      "Model Performance Metrics:\n",
      "MAE: $9.89\n",
      "RMSE: $23.75\n",
      "MAPE: 54.59%\n",
      "R2 Score: 0.292\n",
      "\n",
      "Prediction Interval (68% confidence): ±$23.64\n",
      "Prediction Interval (95% confidence): ±$47.28\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                         feature  importance\n",
      "0                   price_lag_1h    0.644540\n",
      "1                    loss_lag_1h    0.089706\n",
      "4  temperature_rolling_mean_168h    0.077902\n",
      "3                    loss_lag_8h    0.073172\n",
      "5              congestion_lag_7h    0.048666\n",
      "2              congestion_lag_6h    0.037644\n",
      "6               solar_gen_lag_4h    0.028371\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "class MISOPricePredictor:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def load_and_prepare_data(self, generation_path, lmp_path, temp_path):\n",
    "        \"\"\"\n",
    "        Load and prepare data from the three CSV files\n",
    "        generation_path: hourly generation mix data\n",
    "        lmp_path: hourly price data\n",
    "        temp_path: hourly temperature data\n",
    "        \"\"\"\n",
    "        # Load datasets\n",
    "        print(\"Loading datasets...\")\n",
    "        gen_df = pd.read_csv(generation_path)\n",
    "        lmp_df = pd.read_csv(lmp_path)\n",
    "        temp_df = pd.read_csv(temp_path)\n",
    "        \n",
    "        # Convert timestamps\n",
    "        print(\"Processing timestamps...\")\n",
    "        for df in [gen_df, lmp_df, temp_df]:\n",
    "            df['timestamp'] = pd.to_datetime(\n",
    "                df['UTC Timestamp (Interval Ending)'],\n",
    "                format='mixed'\n",
    "            )\n",
    "            \n",
    "        # Filter for 4 full years through December 2023\n",
    "        end_date = '2023-12-31 23:00:00'\n",
    "        start_date = '2020-01-01 00:00:00'\n",
    "        \n",
    "        print(f\"Filtering data from {start_date} to {end_date}...\")\n",
    "        gen_df = gen_df[(gen_df['timestamp'] >= start_date) & (gen_df['timestamp'] <= end_date)]\n",
    "        lmp_df = lmp_df[(lmp_df['timestamp'] >= start_date) & (lmp_df['timestamp'] <= end_date)]\n",
    "        temp_df = temp_df[(temp_df['timestamp'] >= start_date) & (temp_df['timestamp'] <= end_date)]\n",
    "            \n",
    "        # Set index\n",
    "        gen_df.set_index('timestamp', inplace=True)\n",
    "        lmp_df.set_index('timestamp', inplace=True)\n",
    "        temp_df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Sort indices\n",
    "        gen_df.sort_index(inplace=True)\n",
    "        lmp_df.sort_index(inplace=True)\n",
    "        temp_df.sort_index(inplace=True)\n",
    "        \n",
    "        print(\"Extracting data...\")\n",
    "        # Extract generation columns\n",
    "        generation_columns = [\n",
    "            'MISO Total Total Generation (MW)',\n",
    "            'MISO Total Coal Generation (MW)',\n",
    "            'MISO Total Gas Generation (MW)',\n",
    "            'MISO Total Hydro Generation (MW)',\n",
    "            'MISO Total Nuclear Generation (MW)',\n",
    "            'MISO Total Other Generation (MW)',\n",
    "            'MISO Total Wind Generation (MW)',\n",
    "            'MISO Total Solar Generation (MW)',\n",
    "            'MISO Total Storage Generation (MW)'\n",
    "        ]\n",
    "        \n",
    "        # Extract data and fill missing values with 0 for generation\n",
    "        generation_data = gen_df[generation_columns].fillna(0)\n",
    "        temp_data = temp_df['Minneapolis Temperature (Fahrenheit)']\n",
    "        price_data = lmp_df[['Minnesota Hub LMP', 'Minnesota Hub (Congestion)', 'Minnesota Hub (Loss)']]\n",
    "        \n",
    "        # Create complete date range for the period\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "        \n",
    "        # Reindex all dataframes to ensure complete timeline\n",
    "        generation_data = generation_data.reindex(date_range).fillna(0)\n",
    "        temp_data = temp_data.reindex(date_range)\n",
    "        price_data = price_data.reindex(date_range)\n",
    "        \n",
    "        # Create initial DataFrame with only target variable (price)\n",
    "        df = pd.DataFrame({'price': price_data['Minnesota Hub LMP']})\n",
    "        \n",
    "        # Store other variables for creating lagged features\n",
    "        self.historical_data = {\n",
    "            'congestion': price_data['Minnesota Hub (Congestion)'],\n",
    "            'loss': price_data['Minnesota Hub (Loss)'],\n",
    "            'temperature': temp_data,\n",
    "            'total_gen': generation_data['MISO Total Total Generation (MW)'],\n",
    "            'coal_gen': generation_data['MISO Total Coal Generation (MW)'],\n",
    "            'gas_gen': generation_data['MISO Total Gas Generation (MW)'],\n",
    "            'hydro_gen': generation_data['MISO Total Hydro Generation (MW)'],\n",
    "            'nuclear_gen': generation_data['MISO Total Nuclear Generation (MW)'],\n",
    "            'other_gen': generation_data['MISO Total Other Generation (MW)'],\n",
    "            'wind_gen': generation_data['MISO Total Wind Generation (MW)'],\n",
    "            'solar_gen': generation_data['MISO Total Solar Generation (MW)'],\n",
    "            'storage_gen': generation_data['MISO Total Storage Generation (MW)']\n",
    "        }\n",
    "        \n",
    "        # Calculate and store generation percentages for historical features\n",
    "        for gen_type in ['coal', 'gas', 'hydro', 'nuclear', 'other', 'wind', 'solar', 'storage']:\n",
    "            gen_value = self.historical_data[f'{gen_type}_gen']\n",
    "            total_gen = self.historical_data['total_gen']\n",
    "            self.historical_data[f'{gen_type}_pct'] = gen_value / (total_gen + 1e-10) * 100\n",
    "        \n",
    "        print(\"\\nDataset information:\")\n",
    "        print(f\"Time range: {df.index.min()} to {df.index.max()}\")\n",
    "        print(f\"Number of records: {len(df)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self, df, lookback_hours=24):\n",
    "        \"\"\"\n",
    "        Prepare features with hourly lookback periods\n",
    "        lookback_hours: number of hours to look back for all features\n",
    "        \"\"\"\n",
    "        print(\"Creating features...\")\n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Initialize feature dictionaries\n",
    "        time_features = {}\n",
    "        lagged_features = {}\n",
    "        rolling_features = {}\n",
    "        \n",
    "        # Add time-based features\n",
    "        time_features.update({\n",
    "            'hour': df.index.hour,\n",
    "            'day_of_week': df.index.dayofweek,\n",
    "            'month': df.index.month,\n",
    "            'hour_of_day_sin': np.sin(2 * np.pi * df.index.hour / 24),\n",
    "            'hour_of_day_cos': np.cos(2 * np.pi * df.index.hour / 24),\n",
    "            'is_weekend': df.index.dayofweek.isin([5, 6]).astype(int)\n",
    "        })\n",
    "        \n",
    "        print(\"Creating lagged features...\")\n",
    "        # Create lagged features for all historical variables\n",
    "        for feature_name, feature_data in self.historical_data.items():\n",
    "            for i in range(1, lookback_hours + 1):\n",
    "                lagged_features[f'{feature_name}_lag_{i}h'] = feature_data.shift(i)\n",
    "        \n",
    "        # Add lagged price features\n",
    "        for i in range(1, lookback_hours + 1):\n",
    "            lagged_features[f'price_lag_{i}h'] = df['price'].shift(i)\n",
    "        \n",
    "        print(\"Creating rolling statistics...\")\n",
    "        # Calculate rolling statistics for price and other key metrics\n",
    "        windows = [24, 168]  # 24 hours and 1 week\n",
    "        \n",
    "        for feature_name in ['price'] + list(self.historical_data.keys()):\n",
    "            data = df['price'] if feature_name == 'price' else self.historical_data[feature_name]\n",
    "            for window in windows:\n",
    "                rolling_features.update({\n",
    "                    f'{feature_name}_rolling_mean_{window}h': data.rolling(window=window).mean().shift(1),\n",
    "                    f'{feature_name}_rolling_std_{window}h': data.rolling(window=window).std().shift(1)\n",
    "                })\n",
    "        \n",
    "        # Combine all features\n",
    "        feature_df = pd.DataFrame(\n",
    "            {**time_features, **lagged_features, **rolling_features},\n",
    "            index=df.index\n",
    "        )\n",
    "        \n",
    "        # Combine with original price data\n",
    "        result_df = pd.concat([df[['price']], feature_df], axis=1)\n",
    "        \n",
    "        # Handle any infinite values and missing data\n",
    "        result_df = result_df.replace([np.inf, -np.inf], np.nan)\n",
    "        result_df = result_df.dropna()\n",
    "        \n",
    "        print(f\"\\nFinal feature set shape: {result_df.shape}\")\n",
    "        print(f\"Number of features created: {len(result_df.columns) - 1}\")  # -1 for price column\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def split_data(self, df, test_size=0.2):\n",
    "        \"\"\"Split data while preserving time order\"\"\"\n",
    "        train_size = int(len(df) * (1 - test_size))\n",
    "        train_data = df[:train_size]\n",
    "        test_data = df[train_size:]\n",
    "        \n",
    "        feature_columns = [col for col in df.columns if col != 'price']\n",
    "        X_train = train_data[feature_columns]\n",
    "        y_train = train_data['price']\n",
    "        X_test = test_data[feature_columns]\n",
    "        y_test = test_data['price']\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, X_train.columns\n",
    "    \n",
    "    def train_models(self, X_train, y_train):\n",
    "        \"\"\"Train models with optimized parameters\"\"\"\n",
    "        print(\"Training Random Forest model...\")\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            min_samples_split=50,\n",
    "            min_samples_leaf=20,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(\"Training Gradient Boosting model...\")\n",
    "        gb_model = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=8,\n",
    "            min_samples_split=50,\n",
    "            min_samples_leaf=20,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train models\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        gb_model.fit(X_train, y_train)\n",
    "        \n",
    "        return {'Random Forest': rf_model, 'Gradient Boosting': gb_model}\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test, print_results=True):\n",
    "        \"\"\"Model evaluation\"\"\"\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'MAE': mean_absolute_error(y_test, predictions),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),\n",
    "            'MAPE': np.mean(np.abs((y_test - predictions) / y_test)) * 100,\n",
    "            'R2': r2_score(y_test, predictions)\n",
    "        }\n",
    "        \n",
    "        if print_results:\n",
    "            print(\"\\nModel Performance Metrics:\")\n",
    "            print(f\"MAE: ${metrics['MAE']:.2f}\")\n",
    "            print(f\"RMSE: ${metrics['RMSE']:.2f}\")\n",
    "            print(f\"MAPE: {metrics['MAPE']:.2f}%\")\n",
    "            print(f\"R2 Score: {metrics['R2']:.3f}\")\n",
    "            \n",
    "            # Calculate prediction intervals\n",
    "            residuals = y_test - predictions\n",
    "            residual_std = np.std(residuals)\n",
    "            print(f\"\\nPrediction Interval (68% confidence): ±${residual_std:.2f}\")\n",
    "            print(f\"Prediction Interval (95% confidence): ±${2*residual_std:.2f}\")\n",
    "        \n",
    "        return metrics, predictions\n",
    "    \n",
    "    def select_important_features(self, X_train, y_train, feature_names, threshold=0.01):\n",
    "        \"\"\"Select most important features using Random Forest\"\"\"\n",
    "        print(\"Selecting important features...\")\n",
    "        \n",
    "        # Train a random forest for feature selection\n",
    "        selector = RandomForestRegressor(\n",
    "            n_estimators=50,\n",
    "            max_depth=10,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        selector.fit(X_train, y_train)\n",
    "        \n",
    "        # Get feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': selector.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Select features above threshold\n",
    "        important_features = importance[importance['importance'] > threshold]\n",
    "        print(f\"\\nSelected {len(important_features)} important features out of {len(feature_names)}\")\n",
    "        \n",
    "        return important_features.feature.tolist()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = MISOPricePredictor()\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"Loading data...\")\n",
    "    df = predictor.load_and_prepare_data(\n",
    "        'combined_generation_hourly_data.csv',\n",
    "        'miso_lmp_hourly_data_minnesota.csv',\n",
    "        'temp_data_minnesota.csv'\n",
    "    )\n",
    "    \n",
    "    # Prepare features\n",
    "    print(\"Preparing features...\")\n",
    "    df_features = predictor.prepare_features(df)\n",
    "    \n",
    "    # Split data (keeping chronological order)\n",
    "    print(\"Splitting data...\")\n",
    "    train_size = int(len(df_features) * 0.8)\n",
    "    \n",
    "    # Separate features and target\n",
    "    feature_columns = [col for col in df_features.columns if col != 'price']\n",
    "    X = df_features[feature_columns]\n",
    "    y = df_features['price']\n",
    "    \n",
    "    # Create train/test sets\n",
    "    X_train = X[:train_size]\n",
    "    X_test = X[train_size:]\n",
    "    y_train = y[:train_size]\n",
    "    y_test = y[train_size:]\n",
    "    \n",
    "    # Select important features\n",
    "    important_features = predictor.select_important_features(X_train, y_train, feature_columns)\n",
    "    \n",
    "    # Use only important features\n",
    "    X_train = X_train[important_features]\n",
    "    X_test = X_test[important_features]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\nTraining models...\")\n",
    "    models = predictor.train_models(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{name} Evaluation:\")\n",
    "        metrics, predictions = predictor.evaluate_model(model, X_test_scaled, y_test)\n",
    "        \n",
    "        # Print feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            print(\"\\nTop 10 Most Important Features:\")\n",
    "            importance = pd.DataFrame({\n",
    "                'feature': important_features,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            print(importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff011c6-1497-4bee-9800-7efaf2382423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02ec09-7e08-46b7-b309-413681f90c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
